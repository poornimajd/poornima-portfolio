<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Work Experience - Poornima</title>
    <link href="https://fonts.googleapis.com/css2?family=Poppins:wght@300;400;600&display=swap" rel="stylesheet">
    <style>
        body {
            font-family: 'Poppins', sans-serif;
            background: url('background-image.jpg') no-repeat center center fixed;
            background-size: cover;
            color: #333;
            line-height: 1.6;
        }

        nav {
            background-color: #333;
            padding: 15px 0;
            position: fixed;
            width: 100%;
            top: 0;
            z-index: 1000;
            box-shadow: 0 2px 10px rgba(0, 0, 0, 0.1);
        }

        nav ul {
            list-style: none;
            display: flex;
            justify-content: center;
        }

        nav ul li {
            margin: 0 20px;
        }

        nav ul li a {
            color: white;
            text-decoration: none;
            font-weight: 600;
            font-size: 18px;
            transition: color 0.3s ease;
        }

        nav ul li a:hover {
            color: #00bcd4;
        }

        section {
            padding: 100px 20px;
            margin-top: 60px;
            text-align: center;
        }

        h1 {
            font-size: 2.5em;
            margin-bottom: 20px;
            font-weight: 600;
        }

        .card {
            background-color: white;
            box-shadow: 0 4px 10px rgba(0, 0, 0, 0.1);
            border-radius: 10px;
            padding: 20px;
            margin: 20px auto;
            max-width: 800px;
            text-align: left;
            cursor: pointer;
        }

        .card h2 {
            font-size: 1.8em;
            color: #333;
            margin-bottom: 10px;
        }

        .card p {
            color: #666;
            font-size: 1em;
            margin-bottom: 10px;
        }

        /* Modal Styles */
        .modal {
            display: none;
            position: fixed;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            background-color: rgba(0, 0, 0, 0.8);
            z-index: 1001;
            justify-content: center;
            align-items: center;
        }

        .modal-content {
            background-color: white;
            padding: 20px;
            border-radius: 10px;
            width: 60%;
            max-width: 800px;
            max-height: 90%;
            overflow-y: auto;
            text-align: left;
            position: relative;
        }

        .modal-content h2 {
            font-size: 2em;
            color: #333;
        }

        .modal-content ul {
            padding-left: 20px;
        }

        .modal-content ul li {
            margin-bottom: 10px;
            color: #666;
        }

        .close-btn {
            position: absolute;
            top: 10px;
            right: 20px;
            font-size: 1.5em;
            color: #333;
            cursor: pointer;
            z-index: 1002;
        }

        footer {
            text-align: center;
            padding: 20px;
            background-color: #333;
            color: white;
        }
    </style>

<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-GNTR156JSE"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-GNTR156JSE');
</script>

</head>
<body>
    <!-- Navigation Bar -->
    <nav>
        <ul>
            <li><a href="index.html">Home</a></li>
            <li><a href="work.html">Work Experience</a></li>
            <li><a href="projects.html">Projects</a></li>
            <li><a href="certifications.html">Certifications</a></li>
            <li><a href="blogs.html">Blogs</a></li>
        </ul>
    </nav>

    <!-- Work Experience Section -->
    <section>
        <h1>Work Experience</h1>

        <!-- Danfoss Power Solutions -->
        <div class="card" onclick="openModal('modal1')">
            <h2>Danfoss Power Solutions</h2>
            <p>
        <strong>Role:</strong> 
        <a href="#" onclick="openModal('modal1')" style="text-decoration: underline; color: #00bcd4; cursor: pointer;">
            Computer Vision Intern
        </a>
        </p>
            <p><strong>Duration:</strong> May 2024 – Dec 2024</p>
            <p><strong>Tech Stack:</strong> PyTorch, Jetson Orin, ROS1, WandB, Python, OpenCV</p>
        </div>

        <!-- Modal 1 -->
        <div id="modal1" class="modal">
            <div class="modal-content">
                <span class="close-btn" onclick="closeModal('modal1')">&times;</span>
                <h2>Danfoss Power Solutions</h2>
                <h3>Problem Statement:</h3>
                <ul>
                    <li>Build and evaluate a <b>3D person detection</b>b pipeline on multiple stereo cameras (ZED 2i and Multisense S30) to assess performance under diverse environmental conditions. The project aimed to support an underground mining application and provide stakeholders with a cost-performance analysis of the cameras.</li>
                    <li>Research and shortlist <b>MLOps</b> tools for tracking machine learning experiments to reduce technical debt and improve reproducibility.</li>
                </ul>
                <h3>Work Done:</h3>
                <ul>
                    <li>Developed a <b>YOLOv8-based 3D person detection</b> pipeline using depth estimation from stereo cameras. Implemented a heuristic-based algorithm to calculate the centroid distance of detected persons, filtering outliers, optimizing the pipeline for deployment on <b>Jetson Orin</b>.</li>
                    <li>
    Leveraged an open-source <b>mutual information-based targetless auto-calibration method</b> to automatically align <b>stereo cameras</b> and <b>3D LiDAR</b> for multi-sensor fusion. Ensured precise sensor alignment for <b>multi-sensor data fusion</b>, significantly reducing manual efforts and saving $6,000 annually.
</li>
                    <li>Researched 10 MLOps tools and shortlisted <b>Weights & Biases (WandB)</b> based on features and cost. Implemented a custom U-Net model for road and lane segmentation using the CamVid dataset, integrating WandB for experiment tracking and versioning.</li>
                </ul>
                <h3>Challenges Faced:</h3>
                <ul>
                    <li>Encountered <b>outlier points</b> in the ground truth point cloud, requiring robust handling using simple heuristics to improve accuracy.</li>
                    <li>Optimized data retrieval speed from LiDAR and cameras on Jetson Orin to ensure real-time performance.</li>
                    <li>To setup a auto-calibration between different sensors to acheive fusion accuracy comparable to manual methods and managing <b>transformations</b> between different sensors in the Multisense S30,  ZED 2i and and <b>3D LiDAR</b>. 
                </li>
                </ul>
                <h3>Results:</h3>
                <ul>
                    <li>Achieved <b>10ms software-based synchronization accuracy</b> between stereo cameras and 3D LiDAR, improving sensor fusion reliability.</li>
                    <li>Achieved <b><6-pixel reprojection error</b> for automatic calibration between cameras and LiDAR, eliminating manual calibration.</li>
                    <li>Achieved 98% detection accuracy up to 15 meters indoors and 95% accuracy up to 25 meters indoors for both cameras. The Multisense S30 outperformed the ZED 2i in both indoor and limited outdoor test cases.</li>
                    <li>Successfully integrated WandB for <b>tracking, versioning, and automation</b>, improving the team's ability to manage experiments and workflows efficiently.</li>
                </ul>
            </div>
        </div>
<!-- ARTPARK, IISc -->
        <div class="card" onclick="openModal('modal2')">
            <h2>ARTPARK (AI & Robotics Technology Park)</h2>
            <p>
        <strong>Role:</strong> 
        <a href="#" onclick="openModal('modal2')" style="text-decoration: underline; color: #00bcd4; cursor: pointer;">
            Computer Vision & Robotics Engineer
        </a>
        </p>
            <p><strong>Duration:</strong> Apr 2021 – Jul 2023</p>
            <p><strong>Tech Stack:</strong> PyTorch, Jetson Nano, ROS1, ROS2, Python, C++, OpenCV</p>
        </div>

        <!-- Modal 2 -->
<div id="modal2" class="modal">
    <div class="modal-content">
        <span class="close-btn" onclick="closeModal('modal2')">&times;</span>
        <h2>ARTPARK (AI & Robotics Technology Park)</h2>

        <h3>Problem Statement:</h3>
        <ul>
            <li>Develop a novel method to quantify <b>spatially-varying uncertainty</b> associated with external-camera-based pose estimates for autonomous mobile robots in indoor environments.</li>
            <li>Use this uncertainty model as an observation model along with traditional <b>computer vision techniques and sensor fusion</b> for <b>Adaptive Monte Carlo Localization (AMCL)</b> to improve localization accuracy in sparse and dynamic settings.</li>
            <li>Mentor a team of interns to develop <b>deep learning based pose estimation</b> using external CCTV cameras.</li>
            <li>Modularize the legacy AMCL code to improve <b>maintainability</b> and contribute enhancements to the <b>open-source ROS2 repository</b>.</li>
        </ul>

        <h3>Work Done:</h3>
        <ul>
            <li><b>Developed a method to calculate the upper bound on pose estimation errors</b> from external cameras by analyzing errors in <b>homography estimation</b> for flat surfaces, <b>corner point detection accuracy</b>, and <b>camera resolution</b>. This approach included <b>deriving an error formula</b> by differentiating the <b>homography equation</b>, enabling the measurement of how small pixel variations translate into real-world <b>position</b> and <b>angular errors</b>.</li>
            <li><b>Modeled pose uncertainty</b> as a <b>Gaussian distribution</b>, where the mean represents the <b>estimated pose</b> and the variance matrix captures <b>spatially-varying errors</b>. This uncertainty model was seamlessly <b>integrated into the AMCL observation model</b>, leading to a significant improvement in <b>localization accuracy</b>.</li>
            <li>
    Leveraged an open-source <b>Spatio-Temporal Voxel Layer repository</b> for an Intel RealSense camera and 2D onboard LiDAR to enhance obstacle detection and mapping in indoor office environments. Tuned critical navigation parameters, including <b>obstacle inflation radius</b>, <b>costmap resolution</b>, and <b>voxel decay rate</b>, to ensure robust performance in settings with dynamic obstacles, varying lighting conditions, and tight spaces.
</li>
    <li>
        Mentored 4 interns on extending the camera-based localization model to estimate robot pose without markers using deep learning techniques. Guided them in training <b>YOLOv8n</b> for <b>2D keypoint estimation</b> on synthetic data generated from <b>Unity3D</b> and implementing a pipeline to compute pose using <b>PnP</b> from 2D and 3D points. In real-world scenarios, the model performed <b>2D point estimation</b> using YOLO, validated with <b>ground truth</b> data from a motion capture lab.
    </li>
    <li>
        Refactored and modularized the legacy AMCL codebase to improve maintainability and contributed enhancements as an <b>open-source contribution</b> to the <b>ROS2 repository</b>.
    </li>

        </ul>

        <h3>Challenges Faced:</h3>
        <ul>
            <li>Estimating the ground truth pose of the robot from <b>3D LiDAR data</b> to validate the accuracy of the camera-based pose estimates.</li>
            <li>Developing a theoretical formulation to model camera-based pose uncertainty by considering errors from <b>homography estimation, corner detection, and camera resolution</b>.</li>
            <li>Tuning the <b>visual camera-LiDAR navigation</b> system parameters to ensure robust robot performance in dynamic indoor office environments with varying lighting and obstacles.</li>
            <li>Understanding and refactoring the <b>AMCL legacy codebase</b>, which was complex and undocumented.</li>
        </ul>

        <h3>Results:</h3>
        <ul>
            <li>Enhanced robot localization by fusing CCTV camera data with onboard LiDAR using computer vision and particle filter techniques, achieving a <b>60% improvement in localization accuracy</b>.</li>
            <li>Achieved upper-bound error quantification for camera-based pose estimation and verified that <b>errors increase with distance</b> from the camera in both sparse and dynamic environments.</li>
            <li>The project resulted in a <b>paper publication and patent filing</b>, demonstrating the novel approach to uncertainty estimation for camera-based localization.</li>
            <li>Conducted effective pilot trials at the JRD Tata Memorial Library, showcasing the enhanced navigation system,with results presented at the <b>ICRA Workshop 2023</b>.</li>
            <li>Mentored a team of 4 interns who contributed to the next iteration of the robot pose estimation system using deep learning models.</li>
            <li>Modularized the AMCL codebase and contributed the improvements to the <b>ROS2 open-source repository</b>, making it easier to maintain and extend.</li>
        </ul>
    </div>
</div>


        <!-- RBCCPS, IISc -->
        <div class="card" onclick="openModal('modal3')">
            <h2>Robert Bosch Center for Cyber-Physical Systems</h2>
            <p>
        <strong>Role:</strong> 
        <a href="#" onclick="openModal('modal3')" style="text-decoration: underline; color: #00bcd4; cursor: pointer;">
            Computer Vision Intern
        </a>
        </p>
            <p><strong>Duration:</strong> Mar 2020 – Apr 2021</p>
            <p><strong>Tech Stack:</strong> TensorFlow, Jetson Nano, TensorRT, 3D Computer vision</p>
        </div>

        <!-- Modal 3 -->
        <!-- Modal 3 -->
<div id="modal3" class="modal">
    <div class="modal-content">
        <span class="close-btn" onclick="closeModal('modal3')">&times;</span>
        <h2>Robert Bosch Center for Cyber-Physical Systems</h2>

        <h3>Problem Statement:</h3>
        <ul>
            <li>Develop a <b>deep learning-based</b> solution for <b>unmarked Indian road segmentation</b> across various daylight conditions to enhance safety and navigation on Indian roads.</li>
            <li>Build a self-supervised monocular depth estimation pipeline tailored to Indian road conditions, using 3D LiDAR data as ground truth for validation.</li>
            <li>Implement a <b>human pose estimation and compliance detection</b> system to ensure worker safety on construction sites, particularly for detecting compliance on stairways (e.g., whether a person is moving safely up or down stairs).</li>
        </ul>

        <h3>Work Done:</h3>
        <ul>
            <li>Collected custom data from Indian roads using a <b>3D LiDAR sensor</b> as ground truth. The data collection involved manual fusion of the LiDAR and cameras data to ensure accurate ground truth.</li>
            <li>Developed a pipeline for unmarked road segmentation model using ShelfNet model. The model was optimized for runtime on Nvidia Jetson Nano using <b>Nvidia TensorRT</b> to achieve efficient deployment on edge devices.</li>
            <li>Implemented a self-supervised monocular depth estimation pipeline using the <b>Monodepth2 architecture in PyTorch</b>, achieving 85% accuracy within 5-6 meters in outdoor environments. This pipeline was visually compared with stereo matching methods from OpenCV, and the results were found to be comparable.</li>
            <li>Used <b>CenterNet for human pose estimation</b> to estimate the pose of the workers and <b>DeepSORT</b> to track workers on construction sites. Built a heuristic-based compliance detection system to monitor safety on stairways, specifically detecting if a person was moving safely up or down stairs.</li>
            <li>Designed and demonstrated the compliance detection and human pose tracking system to <b>Larsen & Toubro (L&T) construction clients</b>, showing its potential for improving workplace safety.</li>
        </ul>

        <h3>Challenges Faced:</h3>
        <ul>
            <li><b>Ground Truth Collection</b>: Manually calibrating the LiDAR and cameras to ensure accurate depth measurements for validation of the depth estimation pipeline.</li>
            <li> Data availability for Indian roads was limited.</li>
        </ul>

        <h3>Results:</h3>
        <ul>
            <li>Successfully implemented a deep learning-based unmarked Indian road segmentation model, optimized for runtime on Nvidia Jetson Nano, achieving <b>87% accuracy</b> in various daylight conditions.</li>
            <li>Developed a self-supervised monocular depth estimation pipeline in PyTorch, achieving <b>85% accuracy within 5-6 meters outdoors</b>.</li>
            <li>Built a human pose estimation and compliance detection system using CenterNet and heuristics, specifically for monitoring stairway compliance on construction sites.</li>
        </ul>
    </div>
</div>


    </section>

    <!-- Footer -->
    <footer>
        <p>&copy; 2025 Poornima</p>
    </footer>

    <!-- JavaScript -->
    <script>
        function openModal(modalId) {
            document.getElementById(modalId).style.display = 'flex';
        }

        function closeModal(modalId) {
            document.getElementById(modalId).style.display = 'none';
        }

        // Close modal when clicking outside the content
        window.onclick = function(event) {
            let modals = document.getElementsByClassName('modal');
            for (let i = 0; i < modals.length; i++) {
                if (event.target == modals[i]) {
                    modals[i].style.display = 'none';
                }
            }
        }
    </script>
</body>
</html>
