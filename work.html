<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Work Experience - Poornima</title>
    <link href="https://fonts.googleapis.com/css2?family=Poppins:wght@300;400;600&display=swap" rel="stylesheet">
    <style>
        body {
            font-family: 'Poppins', sans-serif;
            background: url('background-image.jpg') no-repeat center center fixed;
            background-size: cover;
            color: #333;
            line-height: 1.6;
        }

        nav {
            background-color: #333;
            padding: 15px 0;
            position: fixed;
            width: 100%;
            top: 0;
            z-index: 1000;
            box-shadow: 0 2px 10px rgba(0, 0, 0, 0.1);
        }

        nav ul {
            list-style: none;
            display: flex;
            justify-content: center;
        }

        nav ul li {
            margin: 0 20px;
        }

        nav ul li a {
            color: white;
            text-decoration: none;
            font-weight: 600;
            font-size: 18px;
            transition: color 0.3s ease;
        }

        nav ul li a:hover {
            color: #00bcd4;
        }

        section {
            padding: 100px 20px;
            margin-top: 60px;
            text-align: center;
        }

        h1 {
            font-size: 2.5em;
            margin-bottom: 20px;
            font-weight: 600;
        }

        .card {
            background-color: white;
            box-shadow: 0 4px 10px rgba(0, 0, 0, 0.1);
            border-radius: 10px;
            padding: 20px;
            margin: 20px auto;
            max-width: 800px;
            text-align: left;
            cursor: pointer;
        }

        .card h2 {
            font-size: 1.8em;
            color: #333;
            margin-bottom: 10px;
        }

        .card p {
            color: #666;
            font-size: 1em;
            margin-bottom: 10px;
        }

        /* Modal Styles */
        .modal {
            display: none;
            position: fixed;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            background-color: rgba(0, 0, 0, 0.8);
            z-index: 1001;
            justify-content: center;
            align-items: center;
        }

        .modal-content {
            background-color: white;
            padding: 20px;
            border-radius: 10px;
            width: 60%;
            max-width: 800px;
            max-height: 90%;
            overflow-y: auto;
            text-align: left;
            position: relative;
        }

        .modal-content h2 {
            font-size: 2em;
            color: #333;
        }

        .modal-content ul {
            padding-left: 20px;
        }

        .modal-content ul li {
            margin-bottom: 10px;
            color: #666;
        }

        .close-btn {
            position: absolute;
            top: 10px;
            right: 20px;
            font-size: 1.5em;
            color: #333;
            cursor: pointer;
            z-index: 1002;
        }

        footer {
            text-align: center;
            padding: 20px;
            background-color: #333;
            color: white;
        }
    </style>
</head>
<body>
    <!-- Navigation Bar -->
    <nav>
        <ul>
            <li><a href="index.html">Home</a></li>
            <li><a href="work.html">Work Experience</a></li>
            <li><a href="projects.html">Projects</a></li>
            <li><a href="certifications.html">Certifications</a></li>
            <li><a href="blogs.html">Blogs</a></li>
        </ul>
    </nav>

    <!-- Work Experience Section -->
    <section>
        <h1>Work Experience</h1>

        <!-- Danfoss Power Solutions -->
        <div class="card" onclick="openModal('modal1')">
            <h2>Danfoss Power Solutions</h2>
            <p><strong>Role:</strong> Machine Learning Co-op</p>
            <p><strong>Duration:</strong> May 2024 – Dec 2024</p>
            <p><strong>Tech Stack:</strong> PyTorch, Jetson Orin, ROS1, WandB, Python, OpenCV</p>
        </div>

        <!-- Modal 1 -->
        <div id="modal1" class="modal">
            <div class="modal-content">
                <span class="close-btn" onclick="closeModal('modal1')">&times;</span>
                <h2>Danfoss Power Solutions</h2>
                <h3>Problem Statement:</h3>
                <ul>
                    <li>Build and evaluate a 3D person detection pipeline on multiple stereo cameras (ZED 2i and Multisense S30) to assess performance under diverse environmental conditions. The project aimed to support an underground mining application and provide stakeholders with a cost-performance analysis of the cameras.</li>
                    <li>Research and shortlist MLOps tools for tracking machine learning experiments to reduce technical debt and improve reproducibility.</li>
                </ul>
                <h3>Work Done:</h3>
                <ul>
                    <li>Developed a YOLOv8-based 3D person detection pipeline using depth estimation from stereo cameras. Implemented a heuristic-based algorithm to calculate the centroid distance of detected persons, optimizing the pipeline for deployment on Jetson Orin.</li>
                    <li>Built an automatic calibration and synchronization system for stereo cameras and 3D LiDAR to ensure accurate sensor alignment and multi-sensor data fusion.</li>
                    <li>Researched 10 MLOps tools and shortlisted Weights & Biases (WandB) based on features and cost. Implemented a custom U-Net model for road and lane segmentation using the CamVid dataset, integrating WandB for experiment tracking and versioning.</li>
                </ul>
                <h3>Challenges Faced:</h3>
                <ul>
                    <li>Encountered outlier points in the ground truth point cloud, requiring robust handling using simple heuristics to improve accuracy.</li>
                    <li>Optimized data retrieval speed from LiDAR and cameras on Jetson Orin to ensure real-time performance.</li>
                    <li>Implemented accurate extrinsics between the RGB and depth cameras in the Multisense S30 to ensure precise depth estimation and sensor alignment.</li>
                </ul>
                <h3>Results:</h3>
                <ul>
                    <li>Achieved 10ms software-based synchronization accuracy between stereo cameras and 3D LiDAR, improving sensor fusion reliability.</li>
                    <li>Achieved <6-pixel reprojection error for automatic calibration between cameras and LiDAR, eliminating manual calibration.</li>
                    <li>Achieved 98% detection accuracy up to 15 meters indoors and 95% accuracy up to 25 meters indoors for both cameras. The Multisense S30 outperformed the ZED 2i in both indoor and limited outdoor test cases.</li>
                    <li>Successfully integrated WandB for tracking, versioning, and automation, improving the team's ability to manage experiments and workflows efficiently.</li>
                </ul>
            </div>
        </div>
<!-- ARTPARK, IISc -->
        <div class="card" onclick="openModal('modal2')">
            <h2>ARTPARK (AI & Robotics Technology Park)</h2>
            <p><strong>Role:</strong> Computer Vision & Robotics Engineer</p>
            <p><strong>Duration:</strong> Apr 2021 – Jul 2023</p>
            <p><strong>Tech Stack:</strong> PyTorch, Jetson Nano, ROS1, ROS2, Python, C++, OpenCV</p>
        </div>

        <!-- Modal 2 -->
<div id="modal2" class="modal">
    <div class="modal-content">
        <span class="close-btn" onclick="closeModal('modal2')">&times;</span>
        <h2>ARTPARK (AI & Robotics Technology Park)</h2>

        <h3>Problem Statement:</h3>
        <ul>
            <li>Develop a novel method to quantify spatially-varying uncertainty associated with external-camera-based pose estimates for autonomous mobile robots in indoor environments.</li>
            <li>Use this uncertainty model as an observation model for Adaptive Monte Carlo Localization (AMCL) to improve localization accuracy in sparse and dynamic settings.</li>
            <li>Mentor a team of interns to build the next version of the uncertainty estimation model for external-camera-based localization.</li>
            <li>Modularize the legacy AMCL code to improve maintainability and contribute enhancements to the open-source ROS2 repository.</li>
            <li>Implement a visual camera-LiDAR navigation system by fusing external camera data with LiDAR to enhance robot localization in indoor office environments.</li>
        </ul>

        <h3>Work Done:</h3>
        <ul>
            <li>Designed a method to calculate the upper bound on pose estimation errors from external cameras by considering errors in homography estimation for flat surfaces, corner point detection accuracy, and camera resolution.</li>
            <li>Derived an error formula by differentiating the homography equation to measure how small pixel variations translate into real-world position and angular errors.</li>
            <li>Modeled the pose uncertainty as a Gaussian distribution, with the mean representing the estimated pose and the variance matrix capturing spatially-varying errors.</li>
            <li>Integrated this uncertainty model into the AMCL observation model, improving localization accuracy.</li>
            <li>Developed a visual navigation system by fusing external camera data with 3D LiDAR for better robot localization in dynamic office environments.</li>
            <li>Tuned navigation parameters for the indoor setting to ensure robustness in environments with varying obstacles and lighting conditions.</li>
            <li>Mentored 4 interns on extending the camera-based localization model and building a pipeline to estimate pose uncertainties.</li>
            <li>Refactored and modularized the legacy AMCL codebase to make it maintainable and contributed improvements to the ROS2 open-source community.</li>
        </ul>

        <h3>Challenges Faced:</h3>
        <ul>
            <li>Estimating the ground truth pose of the robot from 3D LiDAR data to validate the accuracy of the camera-based pose estimates.</li>
            <li>Developing a theoretical formulation to model camera-based pose uncertainty by considering errors from homography estimation, corner detection, and camera resolution.</li>
            <li>Ensuring the uncertainty model accurately represents spatial variations in pose errors as the robot moves further from the camera.</li>
            <li>Tuning the visual camera-LiDAR navigation system parameters to ensure robust robot performance in dynamic indoor office environments with varying lighting and obstacles.</li>
            <li>Understanding and refactoring the AMCL legacy codebase, which was complex and undocumented.</li>
        </ul>

        <h3>Results:</h3>
        <ul>
            <li>Enhanced robot localization by fusing CCTV camera data with onboard LiDAR using computer vision and particle filter techniques, achieving a 60% improvement in localization accuracy.</li>
            <li>Achieved upper-bound error quantification for camera-based pose estimation and verified that errors increase with distance from the camera in both sparse and dynamic environments.</li>
            <li>The project resulted in a paper publication and patent filing, demonstrating the novel approach to uncertainty estimation for camera-based localization.</li>
            <li>Conducted effective pilot trials at the JRD Tata Memorial Library, showcasing the enhanced localization and navigation system.</li>
            <li>Mentored a team of 4 interns who contributed to the next iteration of the robot pose estimation system using camera-LiDAR fusion, with results presented at the ICRA Workshop 2023.</li>
            <li>Achieved robust visual navigation by fusing camera and LiDAR data, tuned for indoor office environments.</li>
            <li>Modularized the AMCL codebase and contributed the improvements to the ROS2 open-source repository, making it easier to maintain and extend.</li>
        </ul>
    </div>
</div>


        <!-- RBCCPS, IISc -->
        <div class="card" onclick="openModal('modal3')">
            <h2>Robert Bosch Center for Cyber-Physical Systems</h2>
            <p><strong>Role:</strong> Technical Associate</p>
            <p><strong>Duration:</strong> Mar 2020 – Apr 2021</p>
            <p><strong>Tech Stack:</strong> TensorFlow, Jetson Nano, TensorRT, 3D Computer vision</p>
        </div>

        <!-- Modal 3 -->
        <!-- Modal 3 -->
<div id="modal3" class="modal">
    <div class="modal-content">
        <span class="close-btn" onclick="closeModal('modal3')">&times;</span>
        <h2>Robert Bosch Center for Cyber-Physical Systems</h2>

        <h3>Problem Statement:</h3>
        <ul>
            <li>Develop a deep learning-based solution for unmarked Indian road segmentation across various daylight conditions to enhance safety and navigation on Indian roads.</li>
            <li>Build a self-supervised monocular depth estimation pipeline tailored to Indian road conditions, using 3D LiDAR data as ground truth for validation.</li>
            <li>Implement a human pose estimation and compliance detection system to ensure worker safety on construction sites, particularly for detecting compliance on stairways (e.g., whether a person is moving safely up or down stairs).</li>
        </ul>

        <h3>Work Done:</h3>
        <ul>
            <li>Collected custom data from Indian roads using a 3D LiDAR sensor as ground truth. The data collection involved manual calibration of the LiDAR and cameras to ensure accurate depth measurements.</li>
            <li>Developed a deep learning-based unmarked road segmentation model using TensorFlow. The model was optimized for runtime on Nvidia Jetson Nano using Nvidia TensorRT to achieve efficient deployment on edge devices.</li>
            <li>Implemented a self-supervised monocular depth estimation pipeline using the Monodepth2 architecture in PyTorch, achieving 85% accuracy within 5-6 meters in outdoor environments. This pipeline was visually compared with stereo matching methods from OpenCV, and the results were found to be comparable.</li>
            <li>Used CenterNet for human pose estimation to track workers on construction sites. Built a heuristic-based compliance detection system to monitor safety on stairways, specifically detecting if a person was moving safely up or down stairs.</li>
            <li>Designed and demonstrated the compliance detection and human pose tracking system to Larsen & Toubro (L&T) construction clients, showing its potential for improving workplace safety.</li>
        </ul>

        <h3>Challenges Faced:</h3>
        <ul>
            <li>Ground Truth Collection: Manually calibrating the LiDAR and cameras to ensure accurate depth measurements for validation of the depth estimation pipeline.</li>
            <li>Depth Estimation Accuracy: Achieving reliable depth estimation using a monocular camera in outdoor environments was challenging due to varying lighting conditions and surface textures.</li>
            <li>Comparing Depth Estimation Methods: It was important to ensure that the self-supervised monocular depth estimation method (Monodepth2) performed comparably to traditional stereo matching methods from OpenCV.</li>
            <li>Compliance Detection: Designing a robust heuristic for detecting compliance on stairways required extensive experimentation and tuning to handle diverse scenarios, such as different stair shapes and lighting conditions.</li>
        </ul>

        <h3>Results:</h3>
        <ul>
            <li>Successfully implemented a deep learning-based unmarked Indian road segmentation model in TensorFlow, optimized for runtime on Nvidia Jetson Nano, achieving 87% accuracy in various daylight conditions.</li>
            <li>Developed a self-supervised monocular depth estimation pipeline in PyTorch, achieving 85% accuracy within 5-6 meters outdoors. The results were visually comparable to traditional stereo matching methods.</li>
            <li>Built a human pose estimation and compliance detection system using CenterNet and heuristics, specifically for monitoring stairway compliance on construction sites.</li>
            <li>Demonstrated the compliance detection and human pose tracking system to Larsen & Toubro (L&T) construction clients, leading to further business engagement.</li>
            <li>Delivered a proof of concept for self-supervised depth estimation and compliance detection, showcasing its potential for improving worker safety and navigation in real-world environments.</li>
        </ul>
    </div>
</div>


    </section>

    <!-- Footer -->
    <footer>
        <p>&copy; 2025 Poornima</p>
    </footer>

    <!-- JavaScript -->
    <script>
        function openModal(modalId) {
            document.getElementById(modalId).style.display = 'flex';
        }

        function closeModal(modalId) {
            document.getElementById(modalId).style.display = 'none';
        }

        // Close modal when clicking outside the content
        window.onclick = function(event) {
            let modals = document.getElementsByClassName('modal');
            for (let i = 0; i < modals.length; i++) {
                if (event.target == modals[i]) {
                    modals[i].style.display = 'none';
                }
            }
        }
    </script>
</body>
</html>

